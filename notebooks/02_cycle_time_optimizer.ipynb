{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ⚡ Cycle Time Optimization Model with Partial Dependence\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "**Objective**: Build a Gradient Boosting model to predict and optimize haul cycle times, with partial dependence analysis for parameter tuning recommendations.\n",
        "\n",
        "**Business Value**: \n",
        "- **15-25% cycle time reduction** through optimized routing and loading\n",
        "- **Data-driven recommendations** for route selection and timing\n",
        "- **Proactive bottleneck avoidance** using predicted cycle times\n",
        "\n",
        "**Model Architecture**: \n",
        "- **Algorithm**: Gradient Boosting Regressor\n",
        "- **Explainability**: Partial Dependence Plots (PDP)\n",
        "- **Output**: Cycle time predictions + optimal parameter recommendations\n",
        "\n",
        "---\n",
        "\n",
        "## Understanding Cycle Time\n",
        "\n",
        "### What Drives Cycle Time?\n",
        "\n",
        "$$\\text{Cycle Time} = T_{load} + T_{haul\\_loaded} + T_{dump} + T_{return\\_empty}$$\n",
        "\n",
        "Each component is affected by different factors:\n",
        "\n",
        "| Phase | Key Factors | Optimization Lever |\n",
        "|-------|-------------|-------------------|\n",
        "| Load | Loader efficiency, queue time | Staggered arrivals |\n",
        "| Haul (Loaded) | Route, grade, traffic | Route selection |\n",
        "| Dump | Dump site congestion | Stockpile management |\n",
        "| Return (Empty) | Route, speed limits | Alternate routes |\n",
        "\n",
        "### Why ML Over Simple Averages?\n",
        "Historical averages miss:\n",
        "- **Time-of-day patterns** - Morning vs afternoon traffic\n",
        "- **Route-specific bottlenecks** - Not all paths equal\n",
        "- **Equipment interactions** - Fleet size effects\n",
        "\n",
        "ML captures these complex relationships for accurate predictions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Snowflake imports\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark import functions as F\n",
        "from snowflake.snowpark.types import *\n",
        "\n",
        "# Snowflake ML imports\n",
        "from snowflake.ml.modeling.ensemble import GradientBoostingRegressor\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.registry import Registry\n",
        "\n",
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Get active session\n",
        "session = get_active_session()\n",
        "print(f\"Connected to: {session.get_current_database()}.{session.get_current_schema()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cycle events data\n",
        "cycles_df = session.table(\"CONSTRUCTION_GEO_DB.RAW.CYCLE_EVENTS\")\n",
        "\n",
        "# Filter to valid cycles\n",
        "valid_cycles = cycles_df.filter(\n",
        "    (F.col(\"CYCLE_TIME_MINUTES\") > 5) &   # Minimum realistic cycle\n",
        "    (F.col(\"CYCLE_TIME_MINUTES\") < 60) &  # Maximum realistic cycle\n",
        "    (F.col(\"LOAD_VOLUME_YD3\") > 0)\n",
        ")\n",
        "\n",
        "print(f\"Total cycle events: {cycles_df.count():,}\")\n",
        "print(f\"Valid cycles: {valid_cycles.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore cycle time distribution\n",
        "stats = valid_cycles.select(\n",
        "    F.avg(\"CYCLE_TIME_MINUTES\").alias(\"avg_cycle_time\"),\n",
        "    F.min(\"CYCLE_TIME_MINUTES\").alias(\"min_cycle_time\"),\n",
        "    F.max(\"CYCLE_TIME_MINUTES\").alias(\"max_cycle_time\"),\n",
        "    F.stddev(\"CYCLE_TIME_MINUTES\").alias(\"std_cycle_time\"),\n",
        "    F.avg(\"LOAD_VOLUME_YD3\").alias(\"avg_volume\"),\n",
        "    F.avg(\"HAUL_DISTANCE_MILES\").alias(\"avg_distance\")\n",
        ").to_pandas()\n",
        "\n",
        "print(\"Cycle Statistics:\")\n",
        "print(stats.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engineer features for cycle time prediction\n",
        "features_df = (valid_cycles\n",
        "    # Time features\n",
        "    .with_column(\"HOUR_OF_DAY\", F.hour(F.col(\"CYCLE_START\")))\n",
        "    .with_column(\"DAY_OF_WEEK\", F.dayofweek(F.col(\"CYCLE_START\")))\n",
        "    .with_column(\"IS_MORNING\", F.when(F.hour(F.col(\"CYCLE_START\")) < 12, F.lit(1)).otherwise(F.lit(0)))\n",
        "    .with_column(\"IS_PEAK_HOUR\", \n",
        "        F.when(\n",
        "            ((F.hour(F.col(\"CYCLE_START\")) >= 7) & (F.hour(F.col(\"CYCLE_START\")) <= 9)) |\n",
        "            ((F.hour(F.col(\"CYCLE_START\")) >= 13) & (F.hour(F.col(\"CYCLE_START\")) <= 14)),\n",
        "            F.lit(1)\n",
        "        ).otherwise(F.lit(0)))\n",
        "    # Route encoding (hash of load + dump locations)\n",
        "    .with_column(\"ROUTE_HASH\", F.hash(F.concat(F.col(\"LOAD_LOCATION\"), F.col(\"DUMP_LOCATION\"))))\n",
        "    # Efficiency metrics\n",
        "    .with_column(\"VOLUME_PER_MILE\", F.col(\"LOAD_VOLUME_YD3\") / F.col(\"HAUL_DISTANCE_MILES\"))\n",
        "    .with_column(\"FUEL_EFFICIENCY\", F.col(\"LOAD_VOLUME_YD3\") / F.col(\"FUEL_CONSUMED_GAL\"))\n",
        ")\n",
        "\n",
        "print(f\"Features engineered: {features_df.count():,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "FEATURE_COLS = [\n",
        "    \"LOAD_VOLUME_YD3\",\n",
        "    \"HAUL_DISTANCE_MILES\",\n",
        "    \"FUEL_CONSUMED_GAL\",\n",
        "    \"HOUR_OF_DAY\",\n",
        "    \"DAY_OF_WEEK\",\n",
        "    \"IS_MORNING\",\n",
        "    \"IS_PEAK_HOUR\",\n",
        "    \"VOLUME_PER_MILE\",\n",
        "    \"FUEL_EFFICIENCY\"\n",
        "]\n",
        "\n",
        "TARGET_COL = \"CYCLE_TIME_MINUTES\"\n",
        "\n",
        "# Prepare training data\n",
        "training_df = features_df.select(FEATURE_COLS + [TARGET_COL]).dropna()\n",
        "\n",
        "print(f\"Training data: {training_df.count():,} rows\")\n",
        "\n",
        "# Split data\n",
        "train_df, test_df = training_df.random_split([0.8, 0.2], seed=42)\n",
        "print(f\"Train: {train_df.count():,}, Test: {test_df.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Gradient Boosting pipeline\n",
        "pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler(\n",
        "            input_cols=FEATURE_COLS,\n",
        "            output_cols=FEATURE_COLS\n",
        "        )),\n",
        "        (\"regressor\", GradientBoostingRegressor(\n",
        "            input_cols=FEATURE_COLS,\n",
        "            label_cols=[TARGET_COL],\n",
        "            output_cols=[\"PREDICTED_CYCLE_TIME\"],\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Cycle Time optimizer model...\")\n",
        "pipeline.fit(train_df)\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "predictions_df = pipeline.predict(test_df)\n",
        "\n",
        "# Calculate metrics\n",
        "results = predictions_df.select(TARGET_COL, \"PREDICTED_CYCLE_TIME\").to_pandas()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(results[TARGET_COL], results[\"PREDICTED_CYCLE_TIME\"])\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(results[TARGET_COL], results[\"PREDICTED_CYCLE_TIME\"])\n",
        "r2 = r2_score(results[TARGET_COL], results[\"PREDICTED_CYCLE_TIME\"])\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"  RMSE: {rmse:.2f} minutes\")\n",
        "print(f\"  MAE:  {mae:.2f} minutes\")\n",
        "print(f\"  R²:   {r2:.4f}\")\n",
        "print(f\"\\n  Avg Actual Cycle Time: {results[TARGET_COL].mean():.2f} minutes\")\n",
        "print(f\"  Avg Predicted Cycle Time: {results['PREDICTED_CYCLE_TIME'].mean():.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Partial Dependence Analysis\n",
        "\n",
        "### Why Partial Dependence for Cycle Time?\n",
        "PDPs answer critical operational questions:\n",
        "- \"How does haul distance affect cycle time?\" (validate expectations)\n",
        "- \"What's the impact of peak hours?\" (quantify traffic effect)\n",
        "- \"Is there a sweet spot for load volume?\" (optimize payload)\n",
        "\n",
        "This enables **data-driven scheduling and route decisions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Partial Dependence for key features\n",
        "from sklearn.inspection import partial_dependence\n",
        "\n",
        "# Get trained model from pipeline\n",
        "gb_model = pipeline.to_sklearn().named_steps['regressor']\n",
        "scaler = pipeline.to_sklearn().named_steps['scaler']\n",
        "\n",
        "# Sample data for PDP\n",
        "sample_size = min(10000, test_df.count())\n",
        "sample_df = test_df.sample(n=sample_size).to_pandas()\n",
        "X_sample = sample_df[FEATURE_COLS]\n",
        "X_scaled = scaler.transform(X_sample)\n",
        "\n",
        "# Key features for PDP analysis\n",
        "PDP_FEATURES = ['HAUL_DISTANCE_MILES', 'LOAD_VOLUME_YD3', 'HOUR_OF_DAY', 'IS_PEAK_HOUR', 'FUEL_EFFICIENCY']\n",
        "\n",
        "# Compute PDPs\n",
        "print(\"Computing Partial Dependence Plots...\")\n",
        "pdp_data = {}\n",
        "\n",
        "for feature in PDP_FEATURES:\n",
        "    feature_idx = FEATURE_COLS.index(feature)\n",
        "    \n",
        "    pdp_result = partial_dependence(\n",
        "        gb_model, \n",
        "        X_scaled, \n",
        "        features=[feature_idx],\n",
        "        kind='both',\n",
        "        grid_resolution=50\n",
        "    )\n",
        "    \n",
        "    pdp_data[feature] = {\n",
        "        'grid_values': pdp_result['grid_values'][0],\n",
        "        'average': pdp_result['average'][0],\n",
        "        'individual': pdp_result['individual'][0]\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✓ {feature}: {len(pdp_result['grid_values'][0])} grid points\")\n",
        "\n",
        "print(f\"\\nPDP computed for {len(PDP_FEATURES)} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Export to ML Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export PDP data to ML.PARTIAL_DEPENDENCE_CURVES\n",
        "MODEL_NAME = \"CYCLE_TIME_OPTIMIZER\"\n",
        "MODEL_VERSION = \"v1.0\"\n",
        "\n",
        "pdp_records = []\n",
        "\n",
        "for feature, data in pdp_data.items():\n",
        "    grid_values = data['grid_values']\n",
        "    avg_predictions = data['average']\n",
        "    ice_curves = data['individual']\n",
        "    \n",
        "    for i, (grid_val, avg_pred) in enumerate(zip(grid_values, avg_predictions)):\n",
        "        ice_at_point = ice_curves[:, i]\n",
        "        \n",
        "        pdp_records.append({\n",
        "            'MODEL_NAME': MODEL_NAME,\n",
        "            'MODEL_VERSION': MODEL_VERSION,\n",
        "            'FEATURE_NAME': feature,\n",
        "            'FEATURE_VALUE': float(grid_val),\n",
        "            'PREDICTED_VALUE': float(avg_pred),\n",
        "            'LOWER_BOUND': float(np.percentile(ice_at_point, 10)),\n",
        "            'UPPER_BOUND': float(np.percentile(ice_at_point, 90)),\n",
        "            'ICE_STD': float(np.std(ice_at_point)),\n",
        "            'SAMPLE_COUNT': int(len(ice_at_point))\n",
        "        })\n",
        "\n",
        "# Delete existing PDP\n",
        "session.sql(f\"DELETE FROM CONSTRUCTION_GEO_DB.ML.PARTIAL_DEPENDENCE_CURVES WHERE MODEL_NAME = '{MODEL_NAME}'\").collect()\n",
        "\n",
        "# Insert records\n",
        "for rec in pdp_records:\n",
        "    session.sql(f\"\"\"\n",
        "        INSERT INTO CONSTRUCTION_GEO_DB.ML.PARTIAL_DEPENDENCE_CURVES \n",
        "        (MODEL_NAME, MODEL_VERSION, FEATURE_NAME, FEATURE_VALUE, PREDICTED_VALUE, \n",
        "         LOWER_BOUND, UPPER_BOUND, ICE_STD, SAMPLE_COUNT)\n",
        "        VALUES ('{rec['MODEL_NAME']}', '{rec['MODEL_VERSION']}', '{rec['FEATURE_NAME']}',\n",
        "                {rec['FEATURE_VALUE']}, {rec['PREDICTED_VALUE']}, {rec['LOWER_BOUND']},\n",
        "                {rec['UPPER_BOUND']}, {rec['ICE_STD']}, {rec['SAMPLE_COUNT']})\n",
        "    \"\"\").collect()\n",
        "\n",
        "print(f\"✅ Exported {len(pdp_records)} PDP data points to ML.PARTIAL_DEPENDENCE_CURVES\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Model Metrics\n",
        "metrics_records = [\n",
        "    {'METRIC_NAME': 'rmse', 'METRIC_VALUE': float(rmse)},\n",
        "    {'METRIC_NAME': 'mae', 'METRIC_VALUE': float(mae)},\n",
        "    {'METRIC_NAME': 'r2_score', 'METRIC_VALUE': float(r2)},\n",
        "    {'METRIC_NAME': 'mse', 'METRIC_VALUE': float(mse)},\n",
        "]\n",
        "\n",
        "session.sql(f\"DELETE FROM CONSTRUCTION_GEO_DB.ML.MODEL_METRICS WHERE MODEL_NAME = '{MODEL_NAME}'\").collect()\n",
        "\n",
        "for rec in metrics_records:\n",
        "    session.sql(f\"\"\"\n",
        "        INSERT INTO CONSTRUCTION_GEO_DB.ML.MODEL_METRICS \n",
        "        (MODEL_NAME, MODEL_VERSION, METRIC_NAME, METRIC_VALUE, METRIC_CONTEXT, SAMPLE_COUNT)\n",
        "        VALUES ('{MODEL_NAME}', '{MODEL_VERSION}', '{rec['METRIC_NAME']}',\n",
        "                {rec['METRIC_VALUE']}, 'test', {len(results)})\n",
        "    \"\"\").collect()\n",
        "\n",
        "print(f\"✅ Exported {len(metrics_records)} metrics to ML.MODEL_METRICS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Register Model & Summary\n",
        "\n",
        "### What We Built\n",
        "- **Model**: Gradient Boosting Regressor for cycle time prediction\n",
        "- **Features**: 9 features including temporal and efficiency metrics\n",
        "- **Output**: Predicted cycle time + PDP analysis\n",
        "\n",
        "### Agent Integration\n",
        "The Route Advisor agent uses this model to:\n",
        "- Predict cycle time for route options\n",
        "- Recommend optimal timing for dispatches\n",
        "- Avoid high-traffic periods\n",
        "\n",
        "### Business Impact\n",
        "- **15-25% cycle time reduction** through ML-driven scheduling\n",
        "- Data-backed recommendations for route selection"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
